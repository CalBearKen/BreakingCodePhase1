A preliminary security analysis of LLM-generated code
As large language models (LLMs) like GPT-4o are increasingly used in software development, assessing the security of their code output has become critical. This study explores the vulnerability landscape of code generated by coding agents and LLMs in response to complex programming tasks.
Method
This is our preliminary study. We used GPT-4o to generate 11 code samples based on unique, non-trivial prompts covering diverse application domains. Each sample was then evaluated using an automated vulnerability detection tool developed by AIonIQ Labs. We will continue to expand our study to other coding agents and LLMs and enhance our study methodology and toolset. In the future, we aim to release our toolsets for the public to use.
